{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoyNtLwmkUVW"
   },
   "source": [
    "# Natural Language Processing. Assignment 1. Tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kygkn1jMkaT9"
   },
   "source": [
    "In this assignment, you need to implement, train, and analyze a Byte-Pair Encoding (BPE) tokenizer.\n",
    "\n",
    "The assignment consist of 3 tasks. When you finish all the tasks, create a GitHub repository for this assignment (you can use this repository later for the other assignments) and submit this notebook in the repository. Leave `requirements.txt` file if your code requires additional installations. Submit the link to the repository in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJokC1PFqhNe"
   },
   "source": [
    "## Task 1: Data Preparation and Vocabulary Size Selection (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUbPSf6pqrcF"
   },
   "source": [
    "First, load the [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus). After loading the corpus, you need to select the appropriate vocabulary size for the BPE tokenizer. The appropriate vocabulary size is the minimal vocabulary size that covers at least 90% of the words in the corpus. The coverage is calculated according to the following formula:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbzsvC77u0We"
   },
   "source": [
    "$$ \\text{coverage}(k) = \\frac{\\sum_{r=1}^{k} f(r)}{\\sum_{r=1}^{N} f(r)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIjntOQDu1ed"
   },
   "source": [
    "where $f(r)$ is the frequency of the top-$r$ word, $k$ is the number of top-$k$ tokens included in vocab, $N$ is the total unique words in corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtunZHVEvNuF"
   },
   "source": [
    "So, for this task you need to do the following:\n",
    "\n",
    "1. Load the Brown corpus (0.5 points)\n",
    "2. Plot cumulative coverage vs. vocabulary size for the loaded corpus (1 point)\n",
    "3. Select the appropriate vocabulary size (0.5 point)\n",
    "4. Answer the questions:\n",
    "    1. Why the coverage slows down the increase as the vocabulary size increases? (0.5 point)\n",
    "    2. Which empirical law explains the slowing down increase of the coverage? (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\olegs\\miniconda3\\Lib\\asyncio\\base_events.py\", line 639, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\olegs\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1985, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\olegs\\miniconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\olegs\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\olegs\\AppData\\Local\\Temp\\ipykernel_21516\\4132429258.py\", line 2, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"c:\\Users\\olegs\\miniconda3\\Lib\\site-packages\\matplotlib\\__init__.py\", line 161, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"c:\\Users\\olegs\\miniconda3\\Lib\\site-packages\\matplotlib\\rcsetup.py\", line 28, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"c:\\Users\\olegs\\miniconda3\\Lib\\site-packages\\matplotlib\\colors.py\", line 57, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale, _image\n",
      "  File \"c:\\Users\\olegs\\miniconda3\\Lib\\site-packages\\matplotlib\\scale.py\", line 39, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"c:\\Users\\olegs\\miniconda3\\Lib\\site-packages\\matplotlib\\ticker.py\", line 144, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"c:\\Users\\olegs\\miniconda3\\Lib\\site-packages\\matplotlib\\transforms.py\", line 48, in <module>\n",
      "    from matplotlib._path import (\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olegs\\miniconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr_name)\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[32m     43\u001b[39m     sys.stderr.write(msg + tb_msg)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m     46\u001b[39m ret = \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "initialization failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[31mImportError\u001b[39m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olegs\\miniconda3\\Lib\\site-packages\\matplotlib\\__init__.py:161\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrcsetup\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cycler  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olegs\\miniconda3\\Lib\\site-packages\\matplotlib\\rcsetup.py:28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_fontconfig_pattern\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_enums\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olegs\\miniconda3\\Lib\\site-packages\\matplotlib\\colors.py:57\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmpl\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _cm, cbook, scale, _image\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_color_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olegs\\miniconda3\\Lib\\site-packages\\matplotlib\\scale.py:39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmpl\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mticker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     40\u001b[39m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[32m     41\u001b[39m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[32m     42\u001b[39m     SymmetricalLogLocator, AsinhLocator, LogitLocator)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mScaleBase\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olegs\\miniconda3\\Lib\\site-packages\\matplotlib\\ticker.py:144\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmpl\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[32m    146\u001b[39m _log = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m    148\u001b[39m __all__ = (\u001b[33m'\u001b[39m\u001b[33mTickHelper\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFormatter\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFixedFormatter\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    149\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mNullFormatter\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFuncFormatter\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFormatStrFormatter\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    150\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mStrMethodFormatter\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mScalarFormatter\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLogFormatter\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    156\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mMultipleLocator\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMaxNLocator\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAutoMinorLocator\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    157\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mSymmetricalLogLocator\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAsinhLocator\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLogitLocator\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olegs\\miniconda3\\Lib\\site-packages\\matplotlib\\transforms.py:48\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_path\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     49\u001b[39m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m     52\u001b[39m DEBUG = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: initialization failed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown', quiet=True)\n",
    "\n",
    "brown_words = [word.lower() for word in brown.words() if word.strip()]\n",
    "word_frequencies = Counter(brown_words)\n",
    "\n",
    "frequencies = np.array(sorted(word_frequencies.values(), reverse=True), dtype=np.int64)\n",
    "cumulative_coverage = np.cumsum(frequencies) / frequencies.sum()\n",
    "vocab_sizes = np.arange(1, len(frequencies) + 1)\n",
    "\n",
    "selected_vocab_size = int(np.searchsorted(cumulative_coverage, 0.9) + 1)\n",
    "selected_coverage = float(cumulative_coverage[selected_vocab_size - 1])\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(vocab_sizes, cumulative_coverage, linewidth=2)\n",
    "plt.axhline(0.9, color='red', linestyle='--', label='90% coverage')\n",
    "plt.axvline(selected_vocab_size, color='green', linestyle='--', label=f'selected k={selected_vocab_size}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Vocabulary size (log scale)')\n",
    "plt.ylabel('Cumulative coverage')\n",
    "plt.title('Brown corpus: cumulative coverage vs vocabulary size')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f'Selected vocabulary size: {selected_vocab_size}')\n",
    "print(f'Coverage at selected size: {selected_coverage:.4f}')\n",
    "print('Why growth slows down: frequent words are covered first, then mostly rare words remain.')\n",
    "print(\"Empirical law: Zipf's law.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94OTtlKZl13I"
   },
   "source": [
    "## Task 2: Implement Byte-Pair Encoding (BPE) Tokenizer (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uphIHqoEmDLH"
   },
   "source": [
    "Implement the [BPE tokenizer](https://arxiv.org/pdf/1508.07909) as the `BPETokenizer` class.\n",
    "\n",
    "The class should contain correctly implemented:\n",
    "\n",
    "* `train` method (1.5 points).\n",
    "* `tokenize` method (1.5 points).\n",
    "\n",
    "The code should have docstrings and comments (1 point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"Byte-Pair Encoding tokenizer with greedy pair merges.\"\"\"\n",
    "\n",
    "    def __init__(self, end_of_word: str = '</w>'):\n",
    "        self.end_of_word = end_of_word\n",
    "        self.merges: List[Tuple[str, str]] = []\n",
    "        self.vocab = set()\n",
    "        self.is_trained = False\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_pair(symbols: Tuple[str, ...], pair: Tuple[str, str]) -> Tuple[str, ...]:\n",
    "        merged = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols) - 1 and symbols[i] == pair[0] and symbols[i + 1] == pair[1]:\n",
    "                merged.append(symbols[i] + symbols[i + 1])\n",
    "                i += 2\n",
    "            else:\n",
    "                merged.append(symbols[i])\n",
    "                i += 1\n",
    "        return tuple(merged)\n",
    "\n",
    "    def train(self, words: List[str], vocab_size: int) -> None:\n",
    "        \"\"\"Train BPE merges until reaching the target vocabulary size.\"\"\"\n",
    "        word_frequencies = Counter(word for word in words if word)\n",
    "        if not word_frequencies:\n",
    "            raise ValueError('Training data is empty.')\n",
    "\n",
    "        split_words: Dict[Tuple[str, ...], int] = {\n",
    "            tuple(list(word) + [self.end_of_word]): freq for word, freq in word_frequencies.items()\n",
    "        }\n",
    "\n",
    "        self.vocab = set()\n",
    "        for symbols in split_words:\n",
    "            self.vocab.update(symbols)\n",
    "\n",
    "        target_vocab_size = max(vocab_size, len(self.vocab))\n",
    "        self.merges = []\n",
    "\n",
    "        while len(self.vocab) < target_vocab_size:\n",
    "            pair_frequencies = Counter()\n",
    "            for symbols, freq in split_words.items():\n",
    "                for i in range(len(symbols) - 1):\n",
    "                    pair_frequencies[(symbols[i], symbols[i + 1])] += freq\n",
    "\n",
    "            if not pair_frequencies:\n",
    "                break\n",
    "\n",
    "            best_pair = pair_frequencies.most_common(1)[0][0]\n",
    "            split_words = {\n",
    "                self._merge_pair(symbols, best_pair): freq for symbols, freq in split_words.items()\n",
    "            }\n",
    "\n",
    "            self.merges.append(best_pair)\n",
    "            self.vocab.add(''.join(best_pair))\n",
    "\n",
    "        self.is_trained = True\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text into BPE subword units.\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise RuntimeError('Tokenizer is not trained.')\n",
    "\n",
    "        output_tokens: List[str] = []\n",
    "        for word in text.split():\n",
    "            symbols = tuple(list(word.lower()) + [self.end_of_word])\n",
    "            for pair in self.merges:\n",
    "                symbols = self._merge_pair(symbols, pair)\n",
    "\n",
    "            for token in symbols:\n",
    "                if token.endswith(self.end_of_word):\n",
    "                    token = token[: -len(self.end_of_word)]\n",
    "                if token:\n",
    "                    output_tokens.append(token)\n",
    "\n",
    "        return output_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhhWPld0zwjG"
   },
   "source": [
    "## Task 3: Tokenizer Training and Analysis (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLkP3Sofz_xV"
   },
   "source": [
    "1. Train the `BPETokenizer` on the Brown corpus with the appropriate vocabulary size selected in Task 1 (1 points)\n",
    "2. Use the Brown corpus (1000 samples) to calculate the mean and standard deviation of\n",
    "    * tokenizer's fertility (1 points)\n",
    "    * length of the tokenized sentence (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer()\n",
    "tokenizer.train(brown_words, vocab_size=selected_vocab_size)\n",
    "\n",
    "brown_sentences = brown.sents()[:1000]\n",
    "fertilities = []\n",
    "tokenized_lengths = []\n",
    "\n",
    "for sentence in brown_sentences:\n",
    "    sentence_words = [word.lower() for word in sentence if word.strip()]\n",
    "    if not sentence_words:\n",
    "        continue\n",
    "\n",
    "    sentence_text = ' '.join(sentence_words)\n",
    "    bpe_tokens = tokenizer.tokenize(sentence_text)\n",
    "\n",
    "    tokenized_lengths.append(len(bpe_tokens))\n",
    "    fertilities.append(len(bpe_tokens) / len(sentence_words))\n",
    "\n",
    "fertility_mean = float(np.mean(fertilities))\n",
    "fertility_std = float(np.std(fertilities))\n",
    "length_mean = float(np.mean(tokenized_lengths))\n",
    "length_std = float(np.std(tokenized_lengths))\n",
    "\n",
    "print(f'Fertility mean: {fertility_mean:.4f}')\n",
    "print(f'Fertility std: {fertility_std:.4f}')\n",
    "print(f'Tokenized sentence length mean: {length_mean:.4f}')\n",
    "print(f'Tokenized sentence length std: {length_std:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Procedure Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the grading of the completed assignments, a random set of students will be sampled for the **offline assignment defence**. The defence will be arranged shortly after the assignment submission deadline. The particular date and time will be announced later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the assignment defence is to ensure the students understand well their own solutions and know how thier solution works. To check this, the students will be asked various questions about the provided solution. In addition, the students will be asked to run their solution to ensure the solution works without errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of questions:\n",
    "\n",
    "1. How the cumulative coverage is calculated? Why is it called cumulative?\n",
    "2. What is the rank of a word?\n",
    "3. How does the BPE tokenizer work? Note: for this question, the students will not be able to see the their own implementation.\n",
    "4. Why do you consider such vocabulary size appropriate?\n",
    "5. What is the formula for the fertility of the tokenizer?\n",
    "6. How do you perform pre-tokenization in your implementation?\n",
    "7. How do you handle stopwords in the training corpus? Why?\n",
    "8. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of the assignment defence, the grade for the assignment may be adjusted."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
